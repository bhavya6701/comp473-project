{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bhavya6701/comp473-project/blob/main/comp473_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VltbyCzGvyd"
   },
   "source": [
    "# Implementation of Artistic Style Transfer Using Convolutional Neural Networks\n",
    "**Authors:** Shibin Koshy [40295019], Ruturajsinh Vihol [40154693], Bhavya Manjibhai Ruparelia [40164863]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the home directory\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "**VGG-16**: A convolutional neural network with 16 layers, popular for style transfer due to its ability to capture detailed hierarchical features across layers. Pre-trained on ImageNet, it offers a balance between depth and computational efficiency.\n",
    "\n",
    "**VGG-19**: An extended version of VGG-16 with 19 layers, providing deeper feature representations. This model can capture more complex details, enhancing style extraction for artistic image synthesis.\n",
    "\n",
    "<!-- **ResNet-50**: A 50-layer residual network with skip connections, which helps retain both high- and low-level features. This architecture is well-suited for extracting intricate textures and patterns in style transfer tasks. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models from torchvision\n",
    "model_dict = {}\n",
    "\n",
    "# VGG-16\n",
    "model_dict[\"vgg-16\"] = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "# VGG-19\n",
    "model_dict[\"vgg-19\"] = models.vgg19(weights=models.VGG19_Weights.DEFAULT)\n",
    "\n",
    "# For each model, freeze all the parameters\n",
    "for model in model_dict.values():\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    # Load in and transform an image, resize it, and convert it to a PyTorch tensor\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Set target size based on max size or shape\n",
    "    if shape is not None:\n",
    "        target_size = shape\n",
    "    else:\n",
    "        target_size = min(max(image.size), max_size)\n",
    "\n",
    "    # Define the transformation pipeline\n",
    "    in_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(target_size),  # Resize to target size\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "            ),  # Normalize\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "content = load_image(HOME + \"/data/input_images/content.jpg\").to(device)\n",
    "style = load_image(HOME + \"/data/input_images/style.jpg\", shape=content.shape[-2:]).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to numpy arrays\n",
    "def tensor_to_image(tensor):\n",
    "    # Invert normalization by reversing the mean and std\n",
    "    denormalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "    )\n",
    "\n",
    "    # Apply denormalization, convert to numpy, squeeze and transpose (C x H x W -> H x W x C)\n",
    "    image = denormalize(tensor).cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "\n",
    "    # Clip values to stay within the [0, 1] range\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "\n",
    "# Display the images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.title.set_text(\"Content Image\")\n",
    "ax1.imshow(tensor_to_image(content), label=\"Content\")\n",
    "\n",
    "ax2.title.set_text(\"Style Image\")\n",
    "ax2.imshow(tensor_to_image(style), label=\"Style\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "\n",
    "    # Iterate through the model layers using named_children()\n",
    "    for name, layer in model.named_children():\n",
    "        # Apply each layer to the image and store the result\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    # Get the batch size, depth, height, and width of the tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    # Reshape the tensor to have the shape (depth, height * width)\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    # Compute the Gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(\"model_config.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extracting data into variables\n",
    "layers = data[\"layers\"]\n",
    "style_weights = data[\"style_weights\"]\n",
    "content_weight = data[\"content_weight\"]\n",
    "style_weight = data[\"style_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Style Transfer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the content loss\n",
    "def calculate_content_loss(target_features, content_features, layer):\n",
    "    return torch.mean((target_features[layer] - content_features[layer]) ** 2)\n",
    "\n",
    "\n",
    "# Function to calculate the style loss\n",
    "def calculate_style_loss(target_features, style_grams, style_weights):\n",
    "    style_loss = 0\n",
    "    for layer, weight in style_weights.items():\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        style_gram = style_grams[layer]\n",
    "        layer_style_loss = weight * (0.25 * torch.mean((target_gram - style_gram) ** 2))\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "    return style_loss\n",
    "\n",
    "\n",
    "# Function to show the images and total loss\n",
    "def plot_images_graph(images, total_losses, steps, checkpoints):\n",
    "    # Create a plot with 2 rows and 6 columns\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "\n",
    "    # Flatten axes for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    for i, image in enumerate(images):\n",
    "        title = (\n",
    "            f\"Iteration {i * (steps // checkpoints):,}\"\n",
    "            if i > 0 and i < len(images) - 1\n",
    "            else \"Initial Content\"\n",
    "            if i == 0\n",
    "            else \"Final Image\"\n",
    "        )\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the total loss values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(total_losses, label=\"Total Loss\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Iterations\")\n",
    "    plt.title(\"Total Loss vs Iterations\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features = {}\n",
    "style_features = {}\n",
    "style_grams = {}\n",
    "\n",
    "# Compute the content and style features for the content and style images\n",
    "content_features[\"vgg-16\"] = extract_features(content, model_dict[\"vgg-16\"].features, layers[\"vgg-16\"])\n",
    "content_features[\"vgg-19\"] = extract_features(content, model_dict[\"vgg-19\"].features, layers[\"vgg-19\"])\n",
    "\n",
    "style_features[\"vgg-19\"] = extract_features(style, model_dict[\"vgg-19\"].features, layers[\"vgg-19\"])\n",
    "style_features[\"vgg-16\"] = extract_features(style, model_dict[\"vgg-16\"].features, layers[\"vgg-16\"])\n",
    "\n",
    "# Compute the gram matrices for the style features\n",
    "style_grams[\"vgg-16\"] = {layer: gram_matrix(style_features[\"vgg-16\"][layer]) for layer in style_features[\"vgg-16\"]}\n",
    "style_grams[\"vgg-19\"] = {layer: gram_matrix(style_features[\"vgg-19\"][layer]) for layer in style_features[\"vgg-19\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function for the style transfer algorithm\n",
    "def style_transfer_algorithm(\n",
    "    model_name,\n",
    "    content_loss_layer,\n",
    "    iterations=2000,\n",
    "    lr=0.003,\n",
    "    checkpoints=10,\n",
    "    alpha=1,\n",
    "    beta=1e-3,\n",
    "):\n",
    "    model = model_dict[model_name]\n",
    "    model_content_features = content_features[model_name]\n",
    "    model_style_grams = style_grams[model_name]\n",
    "    model_style_weights = style_weights[model_name]\n",
    "    model_layers = layers[model_name]\n",
    "\n",
    "    # Ensure that the content tensor is moved to the correct device and requires gradient\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam([target], lr=lr)\n",
    "\n",
    "    # Create a list to store the images and the total loss values\n",
    "    images = []\n",
    "    total_losses = []\n",
    "\n",
    "    # Iterate through the steps\n",
    "    for step in tqdm(range(1, iterations + 1)):\n",
    "        # Extract the features from the target image\n",
    "        target_features = extract_features(target, model.features, model_layers)\n",
    "\n",
    "        # Calculate the content and style loss\n",
    "        content_loss = calculate_content_loss(\n",
    "            target_features, model_content_features, content_loss_layer\n",
    "        )\n",
    "        style_loss = calculate_style_loss(\n",
    "            target_features, model_style_grams, model_style_weights\n",
    "        )\n",
    "\n",
    "        # Compute the total loss\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "        # Update the target image\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the total loss to the list\n",
    "        total_losses.append(total_loss.item())\n",
    "\n",
    "        # Display the images and total loss at the checkpoint steps\n",
    "        if step % (iterations // checkpoints) == 0:\n",
    "            images.append(tensor_to_image(target.detach()))\n",
    "            print(\n",
    "                f\"Step {step}/{iterations} - Total loss: {total_loss.item():.4f}, \"\n",
    "                f\"Content loss: {content_loss.item():.4f}, Style loss: {style_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    # Include the original content image at the start and the final target image at the end\n",
    "    images.insert(0, tensor_to_image(content))\n",
    "    images.append(tensor_to_image(target.detach()))\n",
    "\n",
    "    return images, total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and optimization settings\n",
    "iterations = 2000\n",
    "checkpoints = 10\n",
    "alpha = 1\n",
    "beta = 1e-3\n",
    "lr = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the style transfer function for the VGG-16 model\n",
    "saved_images, total_losses = style_transfer_algorithm(\n",
    "    \"vgg-16\", \"conv4_2\", iterations, lr, checkpoints, alpha, beta\n",
    ")\n",
    "\n",
    "# Create a plot with the images and total loss values\n",
    "plot_images_graph(saved_images, total_losses, iterations, checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-19 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the style transfer function for the VGG-16 model\n",
    "saved_images, total_losses = style_transfer_algorithm(\n",
    "    \"vgg-19\",\n",
    "    \"conv4_3\",\n",
    "    iterations,\n",
    "    lr,\n",
    "    checkpoints,\n",
    ")\n",
    "\n",
    "# Create a plot with the images and total loss values\n",
    "plot_images_graph(saved_images, total_losses, iterations, checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (VGG-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "iterations = 1000\n",
    "alphas = [1, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "betas = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float(\"inf\")\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Iterate through the learning rates\n",
    "for lr in learning_rates:\n",
    "    # Iterate through the alpha values\n",
    "    for alpha in alphas:\n",
    "        # Iterate through the beta values\n",
    "        for beta in betas:\n",
    "            print(f\"-> Learning Rate: {lr} | Alpha: {alpha} | Beta: {beta}\")\n",
    "\n",
    "            # Call the style transfer function for the VGG-16 model\n",
    "            _, total_losses = style_transfer_algorithm(\n",
    "                \"vgg-16\",\n",
    "                \"conv4_2\",\n",
    "                iterations,\n",
    "                lr,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "            # Check if the total loss is the best so far\n",
    "            if total_losses[-1] < best_loss:\n",
    "                best_loss = total_losses[-1]\n",
    "                best_hyperparameters = {\n",
    "                    \"lr\": lr,\n",
    "                    \"alpha\": alpha,\n",
    "                    \"beta\": beta,\n",
    "                }\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the style transfer function for the VGG-19 model with the best hyperparameters\n",
    "saved_images, total_losses = style_transfer_algorithm(\n",
    "    \"vgg-19\",\n",
    "    \"conv4_3\",\n",
    "    iterations,\n",
    "    best_hyperparameters[\"lr\"],\n",
    "    1,\n",
    "    best_hyperparameters[\"alpha\"],\n",
    "    best_hyperparameters[\"beta\"],\n",
    ")\n",
    "\n",
    "# Create a plot with the images and total loss values\n",
    "plot_images_graph(saved_images, total_losses, iterations, checkpoints)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMX6iqVyFbljkm0PR3Plsd",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
