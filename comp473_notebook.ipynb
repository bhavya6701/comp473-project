{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bhavya6701/comp473-project/blob/main/comp473_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VltbyCzGvyd"
   },
   "source": [
    "# Implementation of Artistic Style Transfer Using Convolutional Neural Networks\n",
    "**Authors:** Shibin Koshy [40295019], Ruturajsinh Vihol [40154693], Bhavya Manjibhai Ruparelia [40164863]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the home directory\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "**VGG-16**: A convolutional neural network with 16 layers, popular for style transfer due to its ability to capture detailed hierarchical features across layers. Pre-trained on ImageNet, it offers a balance between depth and computational efficiency.\n",
    "\n",
    "**VGG-19**: An extended version of VGG-16 with 19 layers, providing deeper feature representations. This model can capture more complex details, enhancing style extraction for artistic image synthesis.\n",
    "\n",
    "<!-- **ResNet-50**: A 50-layer residual network with skip connections, which helps retain both high- and low-level features. This architecture is well-suited for extracting intricate textures and patterns in style transfer tasks. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models from torchvision\n",
    "model_dict = {}\n",
    "\n",
    "# VGG-16\n",
    "model_dict[\"vgg-16\"] = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features\n",
    "\n",
    "# VGG-19\n",
    "model_dict[\"vgg-19\"] = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "\n",
    "# For each model, freeze all the parameters\n",
    "for model in model_dict.values():\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    # Load in and transform an image, resize it, and convert it to a PyTorch tensor\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Set target size based on max size or shape\n",
    "    if shape is not None:\n",
    "        target_size = shape\n",
    "    else:\n",
    "        target_size = min(max(image.size), max_size)\n",
    "\n",
    "    # Define the transformation pipeline\n",
    "    in_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "content = load_image(HOME + \"/data/input_images/content8.jpeg\").to(device)\n",
    "style = load_image(HOME + \"/data/input_images/style8.png\", shape=content.shape[-2:]).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images to numpy arrays\n",
    "def tensor_to_image(tensor):\n",
    "    # Invert normalization by reversing the mean and std\n",
    "    denormalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225],\n",
    "    )\n",
    "\n",
    "    # Apply denormalization, convert to numpy, squeeze and transpose (C x H x W -> H x W x C)\n",
    "    image = denormalize(tensor).cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "\n",
    "    # Clip values to stay within the [0, 1] range\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "\n",
    "# Display the images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax1.title.set_text(\"Content Image\")\n",
    "ax1.imshow(tensor_to_image(content), label=\"Content\")\n",
    "\n",
    "ax2.title.set_text(\"Style Image\")\n",
    "ax2.imshow(tensor_to_image(style), label=\"Style\")\n",
    "\n",
    "# Remove the axes\n",
    "ax1.axis(\"off\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Gram Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    \n",
    "    # Iterate through the model layers\n",
    "    for name, layer in model._modules.items():\n",
    "        # Apply each layer to the image and store the result\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    # Get the batch size, depth, height, and width of the tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    # Reshape the tensor to have the shape (depth, height * width)\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    # Compute the Gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(\"model_config.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Extracting data into variables\n",
    "layers = data[\"layers\"]\n",
    "model_style_weights = data[\"style_weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Style Transfer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the content loss\n",
    "def calculate_content_loss(target_features, content_features, layer):\n",
    "    return torch.mean((target_features[layer] - content_features[layer].detach()) ** 2)\n",
    "\n",
    "\n",
    "# Function to calculate the style loss\n",
    "def calculate_style_loss(target_features, style_features, model_style_weights):\n",
    "    style_loss = 0\n",
    "    for layer, weight in model_style_weights.items():\n",
    "        target_feature = target_features[layer]\n",
    "        _, d, h, w = target_feature.shape\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        style_gram = gram_matrix(style_features[layer])\n",
    "        layer_style_loss = weight * (torch.mean((target_gram - style_gram) ** 2))\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "    return style_loss\n",
    "\n",
    "\n",
    "# Function to show the images and total loss\n",
    "def plot_images_graph(images, total_losses, steps, checkpoints):\n",
    "    # Create a plot with 2 rows and 6 columns\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(18, 10))\n",
    "\n",
    "    # Flatten axes for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    for i, image in enumerate(images):\n",
    "        title = (\n",
    "            f\"Iteration {i * (steps // checkpoints):,}\"\n",
    "            if i > 0 and i < len(images) - 1\n",
    "            else \"Initial Image (Noise)\"\n",
    "            if i == 0\n",
    "            else \"Final Image\"\n",
    "        )\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the total loss values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(total_losses, label=\"Total Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Total Loss\")\n",
    "    plt.title(\"Total Loss vs Iteration\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the content and style featuref for the content and style images using the VGG-16 and VGG-19 models\n",
    "content_features = {\n",
    "    \"vgg-16\": extract_features(\n",
    "        content, model_dict[\"vgg-16\"], layers[\"vgg-16\"]\n",
    "    ),\n",
    "    \"vgg-19\": extract_features(\n",
    "        content, model_dict[\"vgg-19\"], layers[\"vgg-19\"]\n",
    "    )\n",
    "}\n",
    "style_features = {\n",
    "    \"vgg-16\": extract_features(style, model_dict[\"vgg-16\"], layers[\"vgg-16\"]),\n",
    "    \"vgg-19\": extract_features(style, model_dict[\"vgg-19\"], layers[\"vgg-19\"])\n",
    "}\n",
    "\n",
    "style_grams = {\n",
    "    \"vgg-16\": {layer: gram_matrix(style_features[\"vgg-16\"][layer]) for layer in style_features[\"vgg-16\"]},\n",
    "    \"vgg-19\": {layer: gram_matrix(style_features[\"vgg-19\"][layer]) for layer in style_features[\"vgg-19\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function for the style transfer algorithm\n",
    "def style_transfer_algorithm(\n",
    "    model_name,\n",
    "    content_loss_layer,\n",
    "    iterations=1000,\n",
    "    lr=0.01,\n",
    "    checkpoints=10,\n",
    "    alpha=1,\n",
    "    beta=1e6,\n",
    "):\n",
    "    model = model_dict[model_name]\n",
    "    model_content_features = content_features[model_name]\n",
    "    model_style_features = style_features[model_name]\n",
    "\n",
    "    # Ensure that the content tensor is moved to the correct device and requires gradient\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam([target], lr=lr)\n",
    "\n",
    "    # Create a list to store the images and the total loss values\n",
    "    images = [tensor_to_image(target.detach())]\n",
    "    total_losses = []\n",
    "\n",
    "    # Iterate through the steps\n",
    "    for step in tqdm(range(1, iterations + 1)):\n",
    "        # Extract the features from the target image\n",
    "        target_features = extract_features(target, model, layers[model_name])\n",
    "\n",
    "        # Calculate the content and style loss\n",
    "        content_loss = calculate_content_loss(\n",
    "            target_features, model_content_features, content_loss_layer\n",
    "        )\n",
    "        style_loss = calculate_style_loss(\n",
    "            target_features, model_style_features, model_style_weights\n",
    "        )\n",
    "\n",
    "        # Compute the total loss\n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "        # Update the target image, zero the gradients, backpropagate, and step the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the total loss to the list\n",
    "        total_losses.append(total_loss.item())\n",
    "\n",
    "        # Save the images at the specified checkpoints\n",
    "        if step % (iterations // checkpoints) == 0:\n",
    "            images.append(tensor_to_image(target.detach()))\n",
    "\n",
    "    print(\n",
    "        f\"Step {step}/{iterations} - Total loss: {total_loss.item():.16f}, \"\n",
    "        f\"Content loss: {content_loss.item():.16f}, Style loss: {style_loss.item():.16f}\"\n",
    "    )\n",
    "\n",
    "    # Include the final image\n",
    "    images.append(tensor_to_image(target.detach()))\n",
    "\n",
    "    return images, total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and optimization settings\n",
    "params = data[\"hyperparameters\"]\n",
    "\n",
    "iterations = params[\"iterations\"]\n",
    "checkpoints = params[\"checkpoints\"]\n",
    "content_weight = params[\"content_weight\"]\n",
    "style_weight = params[\"style_weight\"]\n",
    "lr = params[\"lr\"]\n",
    "\n",
    "content_loss_layer = data[\"content_layer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the style transfer function for the VGG-16 model\n",
    "saved_images, total_losses = style_transfer_algorithm(\n",
    "    \"vgg-16\", content_loss_layer, iterations, lr, checkpoints, 1, 1e6\n",
    ")\n",
    "\n",
    "vgg16_output_image = saved_images[-1]\n",
    "\n",
    "# Create a plot with the images and total loss values\n",
    "plot_images_graph(saved_images, total_losses, iterations, checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-19 Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the style transfer function for the VGG-19 model\n",
    "saved_images, total_losses = style_transfer_algorithm(\n",
    "    \"vgg-19\", content_loss_layer, iterations, lr, checkpoints, content_weight, style_weight\n",
    ")\n",
    "\n",
    "vgg19_output_image = saved_images[-1]\n",
    "\n",
    "# Create a plot with the images and total loss values\n",
    "plot_images_graph(saved_images, total_losses, iterations, checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Content vs Final Style Transfer Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the content, style and final images from VGG-16 and VGG-19\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "axes[0][0].imshow(tensor_to_image(content))\n",
    "axes[0][0].axis(\"off\")\n",
    "axes[0][0].set_title(\"Content Image\")\n",
    "\n",
    "axes[0][1].imshow(tensor_to_image(style))\n",
    "axes[0][1].axis(\"off\")\n",
    "axes[0][1].set_title(\"Style Image\")\n",
    "\n",
    "axes[1][0].imshow(vgg16_output_image)\n",
    "axes[1][0].axis(\"off\")\n",
    "axes[1][0].set_title(\"VGG-16 Final Image\")\n",
    "\n",
    "axes[1][1].imshow(vgg19_output_image)\n",
    "axes[1][1].axis(\"off\")\n",
    "axes[1][1].set_title(\"VGG-19 Final Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
    "layer_images = []\n",
    "\n",
    "# Iterate through the layers\n",
    "for layer in compare_layers: \n",
    "    print(f\"Running style transfer for layer: {layer}\")\n",
    "    \n",
    "    # Call the style transfer function for the VGG-19 model\n",
    "    saved_images, total_losses = style_transfer_algorithm(\n",
    "        \"vgg-19\", layer, iterations, lr, checkpoints, content_weight, style_weight\n",
    "    )\n",
    "\n",
    "    # Append the final image to the list\n",
    "    layer_images.append(saved_images[-2])\n",
    "\n",
    "# Create a plot with all the images obtained using different style_weight values\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i, image in enumerate(layer_images):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Layer {compare_layers[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Weight to Style Weight Ratio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weights = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "output_images = []\n",
    "\n",
    "# Call the style transfer function for the VGG-19 model\n",
    "for cw in content_weights:\n",
    "    print(f\"Running style transfer algorithm with content weight = {cw}\")\n",
    "    saved_images, total_losses = style_transfer_algorithm(\n",
    "        \"vgg-19\",\n",
    "        \"conv4_2\",\n",
    "        500,\n",
    "        lr,\n",
    "        checkpoints,\n",
    "        cw,\n",
    "        style_weight\n",
    "    )\n",
    "    \n",
    "    output_images.append(saved_images[-2])\n",
    "\n",
    "# Create a plot with all the images obtained using different style weight values\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i, image in enumerate(output_images):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Content weight = {content_weights[i]}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMX6iqVyFbljkm0PR3Plsd",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
